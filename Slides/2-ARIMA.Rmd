---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "2&nbsp; ARIMA models"
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 4.3
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache=TRUE,
  warning=FALSE,
  message=FALSE)
library(fpp2)
options(digits=4, width=55)
```

# Stationarity and differencing

## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause

A **stationary series** is:\vspace*{-0.4cm}

 * roughly horizontal
 * constant variance
 * no patterns predictable in the long-term

\pause\vspace*{0.4cm}

\begin{alertblock}{}
\begin{itemize}
\item Transformations (e.g., logs) can help to \textbf{stabilize the variance}.
\item Differences can help to \textbf{stabilize the mean}.
\end{itemize}
\end{alertblock}

## Stationary?
\fontsize{11}{11}\sf

```{r}
dj %>% autoplot() + 
  ylab("Dow Jones Index") + xlab("Day")
```

## Stationary?
\fontsize{11}{11}\sf

```{r}
dj %>% diff() %>% autoplot() + 
  ylab("Change in Dow Jones Index") + xlab("Day")
```

## Stationary?
\fontsize{11}{11}\sf

```{r}
hsales %>% autoplot() + 
  xlab("Year") + ylab("Total sales") +
  ggtitle("Sales of new one-family houses, USA")
```

## Stationary?
\fontsize{11}{11}\sf

```{r}
hsales %>% diff(lag=12) %>% autoplot() +
  xlab("Year") + ylab("Total sales") +
  ggtitle("Seasonal differences of sales of new one-family houses, USA")
```

## Stationary?
\fontsize{11}{11}\sf

```{r}
hsales %>% diff(lag=12) %>% diff(lag=1) %>% autoplot() +
  xlab("Year") + ylab("Total sales") +
  ggtitle("Seasonal differences of sales of new one-family houses, USA")
```

## Electricity production
```{r, echo=TRUE, fig.height=4}
usmelec %>% autoplot()
```

## Electricity production
```{r, echo=TRUE, fig.height=4}
usmelec %>% log() %>% autoplot()
```

## Electricity production

```{r, echo=TRUE, fig.height=3.5}
usmelec %>% log() %>% diff(lag=12) %>%
  autoplot()
```

## Electricity production
```{r, echo=TRUE, fig.height=3.5}
usmelec %>% log() %>% diff(lag=12) %>%
  diff(lag=1) %>% autoplot()
```


# Backshift notation

## Backshift notation

\alert{Backward shift operator}

**Shift back one period**
$$
{B y_{t} = y_{t - 1}} 
$$\pause

**Shift back two periods**:
$$
B(By_{t}) = B^{2}y_{t} = y_{t-2}
$$\pause

**Shift back 12 periods**
$$B^{12}y_{t} = y_{t-12}$$

## Backshift notation

* First differences
$$y'_{t} = y_{t} - y_{t-1} = y_t - By_{t} = (1 - B)y_{t}\: .
$$\pause\vspace*{-0.5cm}
* Second-order differences (i.e., first differences of first differences):
$$y''_{t} = (1 - B)^{2} y_{t}\: .
$$\pause\vspace*{-0.5cm}
* $d$th-order differences:
$$(1 - B)^{d} y_{t}.
$$\pause\vspace*{-0.5cm}
* Seasonal difference followed by first difference:
$$ (1-B)(1-B^m)y_t\: .
$$

# Autoregressive models

## Autoregressive models

\begin{block}{Autoregressive (AR) models:}
$$
  y_{t} = c + \phi_{1}y_{t - 1} + \phi_{2}y_{t - 2} + \cdots + \phi_{p}y_{t - p} + \varepsilon_{t},
$$
where $\varepsilon_t$ is white noise. This is a multiple regression with \textbf{lagged values} of $y_t$ as predictors.
\end{block}

```{r arp, echo=FALSE, fig.height=3}
set.seed(1)
p1 <- autoplot(10 + arima.sim(list(ar = -0.8), n = 100)) +
  ylab("") + ggtitle("AR(1)")
p2 <- autoplot(20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100)) +
  ylab("") + ggtitle("AR(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```

## Stationarity conditions

We normally restrict autoregressive models to stationary data, and then some constraints on the values of the parameters are required.

\begin{block}{General condition for stationarity}
Complex roots of $1-\phi_1 z - \phi_2 z^2 - \dots - \phi_pz^p$ lie outside the unit circle on the complex plane.
\end{block}\pause

* For $p=1$: $-1<\phi_1<1$.
* For $p=2$:\newline $-1<\phi_2<1\qquad \phi_2+\phi_1 < 1 \qquad \phi_2 -\phi_1 < 1$.
* More complicated conditions hold for $p\ge3$.

# Moving Average models

## Moving Average (MA) models

\begin{block}{Moving Average (MA) models:}
$$
  y_{t} = c + \varepsilon_t + \theta_{1}\varepsilon_{t - 1} + \theta_{2}\varepsilon_{t - 2} + \cdots + \theta_{q}\varepsilon_{t - q},
$$
where $\varepsilon_t$ is white noise.
This is a multiple regression with \textbf{past \emph{errors}}
as predictors. \emph{Don't confuse this with moving average smoothing!}
\end{block}

```{r maq, fig.height=2.5, echo=FALSE}
set.seed(2)
p1 <- autoplot(20 + arima.sim(list(ma = 0.8), n = 100)) +
  ylab("") + ggtitle("MA(1)")
p2 <- autoplot(arima.sim(list(ma = c(-1, +0.8)), n = 100)) +
  ylab("") + ggtitle("MA(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```


## Invertibility

* Invertible models have property that distant past has negligible effect on forecasts. Requires consraints on MA parameters.

\begin{block}{General condition for invertibility}
Complex roots of $1+\theta_1 z + \theta_2 z^2 + \dots + \theta_qz^q$ lie outside the unit circle on the complex plane.
\end{block}\pause\vspace*{-0.3cm}

* For $q=1$:  $-1<\theta_1<1$.
* For $q=2$:\newline $-1<\theta_2<1\qquad \theta_2+\theta_1 >-1 \qquad \theta_1 -\theta_2 < 1$.
* More complicated conditions hold for $q \ge 3$.

# Non-seasonal ARIMA models

## ARIMA models

\begin{block}{Autoregressive Moving Average models:}\vspace*{-0.2cm}
\begin{align*}
y_{t} &= c + \phi_{1}y_{t - 1} + \cdots + \phi_{p}y_{t - p} \\
& \hspace*{2.4cm}\text{} + \theta_{1}e_{t - 1} + \cdots + \theta_{q}e_{t - q} + e_{t}.
\end{align*}
\end{block}\pause

* Predictors include both **lagged values of $y_t$ and lagged errors.**
* Conditions on coefficients ensure stationarity.
* Conditions on coefficients ensure invertibility.
\pause

### Autoregressive Integrated Moving Average models
* Combine ARMA model with **differencing**.
* $(1-B)^d y_t$ follows an ARMA model.

## ARIMA models

\alert{Autoregressive Integrated Moving Average models}
\begin{block}{ARIMA($p, d, q$) model}
\begin{tabular}{rl}
AR:& $p =$ order of the autoregressive part\\
I: & $d =$ degree of first differencing involved\\
MA:& $q =$ order of the moving average part.
\end{tabular}
\end{block}

* White noise model: ARIMA(0,0,0)
* Random walk: ARIMA(0,1,0) with no constant
* Random walk with drift: ARIMA(0,1,0) with \rlap{const.}
* AR($p$): ARIMA($p$,0,0)
* MA($q$): ARIMA(0,0,$q$)

## Backshift notation for ARIMA

* ARIMA($p,q$) model:\vspace*{-1cm}\newline
\parbox{12cm}{\small\begin{align*}
\hspace*{-1cm}
y_{t} &= c + \phi_{1}y_{t - 1} + \cdots + \phi_{p}y_{t - p} + \theta_{1}\varepsilon_{t - 1} + \cdots + \theta_{q}\varepsilon_{t - q} + \varepsilon_{t} \\
\hspace*{-1cm}
y_{t} &= c + \phi_{1}By_{t} + \cdots + \phi_pB^py_{t}
           + \varepsilon_{t} + \theta_{1}B\varepsilon_{t} + \cdots + \theta_qB^q\varepsilon_{t} \\
\hspace*{-1cm}
\text{or}\quad & (1-\phi_1B - \cdots - \phi_p B^p) y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t
\end{align*}}

* ARIMA(1,1,1) model:
\[
\begin{array}{c c c c}
(1 - \phi_{1} B) & (1 - B) y_{t} &= &c + (1 + \theta_{1} B) \varepsilon_{t}\\
{\uparrow} & {\uparrow}   &   &{\uparrow}\\
{\text{AR(1)}} & {\text{First}}   &     &{\text{MA(1)}}\\
& {\hbox to 0cm{\hss\text{difference}\hss}}\\
\end{array}
\]\pause
Written out:
$$y_t =  c + y_{t-1} + \phi_1 y_{t-1}- \phi_1 y_{t-2} + \theta_1\varepsilon_{t-1} + \varepsilon_t $$

## R model

\fontsize{13}{16}\sf

\begin{block}{Intercept form}
\centerline{$(1-\phi_1B - \cdots - \phi_p B^p) y_t' = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t$}
\end{block}

\begin{block}{Mean form}
\centerline{$(1-\phi_1B - \cdots - \phi_p B^p)(y_t' - \mu) = (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t$}
\end{block}

 * $y_t' = (1-B)^d y_t$
 * $\mu$ is the mean of $y_t'$.
 * $c = \mu(1-\phi_1 - \cdots - \phi_p )$.
 * R uses mean form.

## US personal consumption
\fontsize{11}{11}\sf
```{r, fig.height=3.8}
autoplot(uschange[,"Consumption"]) +
  xlab("Year") + ylab("Quarterly percentage change") +
  ggtitle("US consumption")
```

## US personal consumption
\fontsize{11}{13}\sf

```{r, echo=TRUE}
(fit <- auto.arima(uschange[,"Consumption"]))
```

```{r usconsumptioncoefs, echo=FALSE}
coef <- coefficients(fit)
intercept <- coef['intercept'] * (1-coef['ar1'] - coef['ar2'])
```

```{r, include=FALSE}
# Following line assumes forecast v8.4+
if(!identical(arimaorder(fit),c(p=2L,d=0L,q=2L)))
  stop("Different model from expected")
```

\pause\vfill

### ARIMA(2,0,2) model:
\centerline{$
  y_t = c + `r format(coef['ar1'], nsmall=3, digits=3)`y_{t-1}
          `r format(coef['ar2'], nsmall=3, digits=3)` y_{t-2}
          `r format(coef['ma1'], nsmall=3, digits=3)` \varepsilon_{t-1}
          + `r format(coef['ma2'], nsmall=3, digits=3)` \varepsilon_{t-2}
          + \varepsilon_{t},
$}
where $c= `r format(coef['intercept'], nsmall=3, digits=3)` \times (1 - `r format(coef['ar1'], nsmall=3, digits=3)` + `r format(-coef['ar2'], nsmall=3, digits=3)`) = `r format(intercept, nsmall=3, digits=3)`$
and $\varepsilon_t \sim N(0,`r format(fit$sigma2, nsmall=3, digits=3)`)$.

## US personal consumption
\fontsize{13}{14}\sf

```{r, echo=TRUE, fig.height=4}
fit %>% forecast(h=10) %>% autoplot(include=80)
```

## Information criteria

\begin{block}{Akaike's Information Criterion (AIC):}
\centerline{$\text{AIC} = -2 \log(L) + 2(p+q+k+1),$}
where $L$ is the likelihood of the data,\newline
$k=1$ if $c\ne0$ and $k=0$ if $c=0$.
\end{block}\pause
\begin{block}{Corrected AIC:}
\[
\text{AICc} = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.
\]
\end{block}\pause
\begin{alertblock}{}Good models are obtained by minimizing the \text{AICc}.\end{alertblock}

## How does auto.arima() work?

\begin{block}{A non-seasonal ARIMA process}
\[
\phi(B)(1-B)^dy_{t} = c + \theta(B)\varepsilon_t
\]
Need to select appropriate orders: \alert{$p,q, d$}
\end{block}

\alert{Hyndman and Khandakar (JSS, 2008) algorithm:}

  * Select no.\ differences \alert{$d$} and \alert{$D$} via KPSS test and seasonal strength measure.
  * Select \alert{$p,q$} by minimising AICc.
  * Use stepwise search to traverse model space.

## How does auto.arima() work?
\fontsize{12.5}{14}\sf

Step 1:
:  Select values of $d$ and $D$.

Step 2:
:  Select current model (with smallest AICc) from:\newline
ARIMA$(2,d,2)$\newline
ARIMA$(0,d,0)$\newline
ARIMA$(1,d,0)$\newline
ARIMA$(0,d,1)$
\pause\vspace*{-0.1cm}

Step 3:
:  Consider variations of current model:\vspace*{-0.2cm}

    * vary one of $p,q,$ from current model by \rlap{$\pm1$;}
    * $p,q$ both vary from current model by $\pm1$;
    * Include/exclude $c$ from current model.

  Model with lowest AICc becomes current model.

\begin{block}{}Repeat Step 3 until no lower AICc can be found.\end{block}

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
autoplot(internet)
```

## Choosing an ARIMA model
\fontsize{12}{13}\sf

```{r, echo=TRUE, fig.height=4}
(fit <- auto.arima(internet))
```

## Choosing an ARIMA model
\fontsize{12}{13}\sf

```{r, echo=TRUE, fig.height=4}
(fit <- auto.arima(internet, stepwise=FALSE,
  approximation=FALSE))
```

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit, plot=TRUE)
```

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
fit %>% forecast() %>% autoplot()
```

# Seasonal ARIMA models

## Seasonal ARIMA models

| ARIMA | $~\underbrace{(p, d, q)}$ | $\underbrace{(P, D, Q)_{m}}$ |
| ----: | :-----------------------: | :--------------------------: |
|       | ${\uparrow}$              | ${\uparrow}$                 |
|       | Non-seasonal part         | Seasonal part of             |
|       | of the model              | of the model                 |

where $m =$ number of observations per year.

## Seasonal ARIMA models

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$  model (without constant)\pause
$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.
$$\pause

\setlength{\unitlength}{1mm}
\begin{footnotesize}
\begin{picture}(100,25)(-5,0)
\thinlines
{\put(5,22){\vector(0,1){6}}}
{\put(22,10){\vector(0,1){18}}}
{\put(38,22){\vector(0,1){6}}}
{\put(52,10){\vector(0,1){18}}}
{\put(77,22){\vector(0,1){6}}}
{\put(95,10){\vector(0,1){18}}}
{\put(-10,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{AR(1)}
                    \end{array}\right)$}}
{\put(12,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{AR(1)}
                    \end{array}\right)$}}
{\put(25,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{difference}
                    \end{array}\right)$}}
{\put(40,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{difference}
                    \end{array}\right)$}}
{\put(65,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{MA(1)}
                    \end{array}\right)$}}
{\put(85,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{MA(1)}
                    \end{array}\right)$}}
\end{picture}
\end{footnotesize}

\vspace*{10cm}

## Seasonal ARIMA models

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$  model (without constant)
$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.
$$

All the factors can be multiplied out and the general model
written as follows:
\begin{align*}
y_{t}  &= (1 + \phi_{1})y_{t - 1} - \phi_1y_{t-2} + (1 + \Phi_{1})y_{t - 4}\\
&\text{}
 -  (1  + \phi_{1}  +  \Phi_{1} + \phi_{1}\Phi_{1})y_{t - 5}
 +  (\phi_{1}  +  \phi_{1} \Phi_{1}) y_{t - 6} \\
& \text{}  - \Phi_{1} y_{t - 8} +  (\Phi_{1}  +  \phi_{1} \Phi_{1}) y_{t - 9}
  - \phi_{1} \Phi_{1} y_{t  -  10}\\
  &\text{}
+    \varepsilon_{t} + \theta_{1}\varepsilon_{t - 1} + \Theta_{1}\varepsilon_{t - 4}  + \theta_{1}\Theta_{1}\varepsilon_{t - 5}.
\end{align*}
\vspace*{10cm}

## European quarterly retail trade

```{r, echo=TRUE, fig.height=3.6}
autoplot(euretail) +
  xlab("Year") + ylab("Retail index")
```

## European quarterly retail trade
\fontsize{11}{12}\sf

```{r euretail, echo=TRUE}
(fit <- auto.arima(euretail))
```
## European quarterly retail trade
\fontsize{11}{12}\sf

```{r euretail2, echo=TRUE}
(fit <- auto.arima(euretail, stepwise=TRUE,
  approximation=FALSE))
```

## European quarterly retail trade
\fontsize{13}{13}\sf

```{r, dependson='euretail2'}
checkresiduals(fit, test=FALSE)
```

## European quarterly retail trade
\fontsize{13}{13}\sf

```{r, dependson='euretail2'}
forecast(fit, h=36) %>% autoplot()
```

## Cortecosteroid drug sales

```{r h02, echo=FALSE}
lh02 <- log(h02)
tmp <- cbind("H02 sales (million scripts)" = h02,
             "Log H02 sales"=lh02)
autoplot(tmp, facets=TRUE) + xlab("Year") + ylab("")
```

## Cortecosteroid drug sales
\fontsize{10}{14}\sf

```{r h02tryharder, echo=TRUE, fig.height=3.6}
(fit <- auto.arima(h02, lambda=0, max.order=9,
  stepwise=FALSE, approximation=FALSE))
```

## Cortecosteroid drug sales
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4, dependson='h02tryharder'}
checkresiduals(fit)
```


## Understanding ARIMA models
\fontsize{14}{16}\sf

\begin{alertblock}{Long-term forecasts}
\centering\begin{tabular}{lll}
zero & $c=0,d+D=0$\\
non-zero constant & $c=0,d+D=1$ & $c\ne0,d+D=0$  \\
linear & $c=0,d+D=2$ & $c\ne0,d+D=1$ \\
quadratic & $c=0,d+D=3$ & $c\ne0,d+D=2$ \\
\end{tabular}
\end{alertblock}

### Forecast variance and $d+D$
  * The higher the value of $d+D$, the more rapidly the prediction intervals increase in size.
  * For $d+D=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.

## Prediction intervals

* Prediction intervals **increase in size with forecast horizon**.
* Calculations assume residuals are **uncorrelated** and **normally distributed**.
* Prediction intervals tend to be too narrow.
    * the uncertainty in the parameter estimates has not been accounted for.
    * the ARIMA model assumes historical patterns will not change during the forecast period.
    * the ARIMA model assumes uncorrelated future errors


# Lab Session 3
##
\fontsize{48}{60}\sf\centering
**Lab Session 3**

