---
title: "Forecasting: principles and practice"
author: "Rob J Hyndman"
date: "2.3&nbsp; Stationarity and differencing"
fontsize: 14pt
output:
  beamer_presentation:
    fig_width: 7
    fig_height: 4.3
    highlight: tango
    theme: metropolis
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  cache=TRUE,
  warning=FALSE,
  message=FALSE)
library(fpp2)
options(digits=4, width=55)
```

# Stationarity

## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause

A \textbf{stationary series} is:

*  roughly horizontal
*  constant variance
*  no patterns predictable in the long-term

## Stationary?

```{r, echo=FALSE}
autoplot(dj) + ylab("Dow Jones Index") + xlab("Day")
```

## Stationary?

```{r, echo=FALSE}
autoplot(diff(dj)) + ylab("Change in Dow Jones Index") + xlab("Day")
```

## Stationary?

```{r, echo=FALSE}
autoplot(strikes) + ylab("Number of strikes") + xlab("Year")
```

## Stationary?

```{r, echo=FALSE}
autoplot(hsales) + xlab("Year") + ylab("Total sales") +
  ggtitle("Sales of new one-family houses, USA")
```

## Stationary?
```{r, echo=FALSE}
autoplot(eggs) + xlab("Year") + ylab("$") +
  ggtitle("Price of a dozen eggs in 1993 dollars")
```

## Stationary?
```{r, echo=FALSE}
autoplot(window(pigs/1e3, start=1990)) + xlab("Year") + ylab("thousands") +
  ggtitle("Number of pigs slaughtered in Victoria")
```

## Stationary?

```{r, echo=FALSE}
autoplot(lynx) + xlab("Year") + ylab("Number trapped") +
  ggtitle("Annual Canadian Lynx Trappings")
```

## Stationary?

```{r, echo=FALSE}
autoplot(window(ausbeer, start=1992)) + xlab("Year") + ylab("megalitres") +
  ggtitle("Australian quarterly beer production")
```

## Stationarity

\begin{block}{Definition}
If $\{y_t\}$ is a stationary time series, then for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not depend on $t$.
\end{block}\pause\vspace*{0.4cm}

Transformations help to \textbf{stabilize the variance}.

For ARIMA modelling, we also need to \textbf{stabilize the mean}.

## Non-stationarity in the mean
\alert{Identifying non-stationary series}

* time plot.

* The ACF of stationary data drops to zero relatively quickly
* The ACF of non-stationary data decreases slowly.
* For non-stationary data, the value of $r_1$ is often
     large and positive.

## Example: Dow-Jones index
\fontsize{11}{11}\sf

```{r}
autoplot(dj) + ylab("Dow Jones Index") + xlab("Day")
```

## Example: Dow-Jones index
\fontsize{11}{11}\sf

```{r}
ggAcf(dj)
```

## Example: Dow-Jones index
\fontsize{11}{11}\sf

```{r}
autoplot(diff(dj)) +
  ylab("Change in Dow Jones Index") + xlab("Day")
```

## Example: Dow-Jones index
\fontsize{11}{11}\sf

```{r}
ggAcf(diff(dj))
```

# Differencing

## Differencing

* Differencing helps to \textbf{stabilize the mean}.
* The differenced series is the \emph{change} between each observation
in the original series: ${y'_t = y_t - y_{t-1}}$.
* The differenced series will have only $T-1$ values since it is not possible to calculate a difference $y_1'$ for the first observation.

## Second-order  differencing

Occasionally the differenced data will not appear stationary and it
may be necessary to difference the data a second time:\pause
\begin{align*}
y''_{t}  &=  y'_{t}  - y'_{t - 1} \\
&= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
&= y_t - 2y_{t-1} +y_{t-2}.
\end{align*}\pause\vspace*{-0.75cm}

* $y_t''$ will have  $T-2$  values.
* In practice,  it is almost never necessary to go beyond second-order
differences.

## Seasonal differencing

A seasonal difference is the difference between an observation and the corresponding observation from the previous year.\pause
$${y'_t = y_t - y_{t-m}}$$
where $m=$ number of seasons.\pause

* For monthly data $m=12$.
* For quarterly data $m=4$.

## Electricity production
```{r, echo=TRUE, fig.height=4}
usmelec %>% autoplot()
```

## Electricity production
```{r, echo=TRUE, fig.height=4}
usmelec %>% log() %>% autoplot()
```

## Electricity production

```{r, echo=TRUE, fig.height=3.5}
usmelec %>% log() %>% diff(lag=12) %>%
  autoplot()
```

## Electricity production
```{r, echo=TRUE, fig.height=3.5}
usmelec %>% log() %>% diff(lag=12) %>%
  diff(lag=1) %>% autoplot()
```

## Electricity production

* Seasonally differenced series is closer to being stationary.
* Remaining non-stationarity  can be removed with further first difference.

If $y'_t = y_t - y_{t-12}$ denotes seasonally differenced series, then twice-differenced series is\vspace*{-0.1cm}
\begin{block}{}\vspace*{-0.2cm}
\begin{align*}
y^*_t &= y'_t - y'_{t-1} \\
      &= (y_t - y_{t-12}) - (y_{t-1} - y_{t-13}) \\
      &= y_t - y_{t-1} - y_{t-12} + y_{t-13}\: .
\end{align*}
\end{block}\vspace*{10cm}

## Seasonal differencing
\fontsize{14}{15}\sf

When both seasonal and first differences are applied\dots\pause

* it makes no difference
which is done first---the result will be the same.
* If seasonality is strong, we recommend that seasonal differencing be done first because sometimes the resulting series will be stationary and there will be no need for further first difference.
\pause

###
It is important that if differencing is used, the differences are
interpretable.

## Interpretation of differencing

* first differences are the change between \textbf{one observation and the
next};
* seasonal differences are the change between \textbf{one year to the
next}.
\pause

But taking lag 3 differences for yearly data, for example, results in a model which cannot be sensibly interpreted.

# Unit root tests
## Unit root tests

\alert{Statistical tests to determine the required order of differencing.}

1. Augmented Dickey Fuller test: null hypothesis is that the data are non-stationary and non-seasonal.
2. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: null  hypothesis is that the data are stationary and non-seasonal.
3. Other tests available for seasonal data.

## KPSS test
\fontsize{10}{11}\sf

```{r, echo=TRUE}
library(urca)
summary(ur.kpss(goog))
```

\pause

```{r, echo=TRUE}
ndiffs(goog)
```

## Automatically selecting differences

STL decomposition: $y_t = T_t+S_t+R_t$

Seasonal strength $F_s = \max\big(0, 1-\frac{\text{Var}(R_t)}{\text{Var}(S_t+R_t)}\big)$

If $F_s > 0.64$, do one seasonal difference.

\pause\fontsize{12}{15}\sf\vspace*{1cm}

```{r, echo=TRUE}
usmelec %>% log() %>% nsdiffs()
usmelec %>% log() %>% diff(lag=12) %>% ndiffs()
```

# Lab session 15
##
\fontsize{48}{60}\sf\centering
**Lab Session 15**

# Backshift notation

## Backshift notation

A  very useful notational device is the backward  shift operator,  $B$,  which is used as follows:
$$
{B y_{t}  =  y_{t - 1}} \: .
$$\pause
In  other  words,   $B$,  operating on  $y_{t}$,   has  the
effect   of   \textbf{shifting  the  data  back  one   period}. \pause
Two applications of  $B$  to  $y_{t}$ \textbf{shifts the data  back  two
periods}:
$$
B(By_{t})  =  B^{2}y_{t}  =  y_{t-2}\: .
$$\pause
For  monthly  data, if we wish to shift attention  to  ``the
same  month last year,''  then  $B^{12}$
is used,  and  the
notation is  $B^{12}y_{t}$  =  $y_{t-12}$.

## Backshift notation

The   backward   shift  operator  is  convenient   for describing  the
process  of  \textit{differencing}. \pause
A first difference can be written as
$$
y'_{t}  =  y_{t} -   y_{t-1} = y_t - By_{t}  =  (1 - B)y_{t}\: .
$$\pause
Note  that a first difference is represented by  $(1 -  B)$.
\pause

Similarly,   if second-order differences (i.e.,   first
differences  of  first differences) have  to  be  computed,
then:
\[
y''_{t}  =  y_{t} -   2y_{t - 1}  +  y_{t - 2} = (1 - B)^{2} y_{t}\: .
\]

## Backshift notation

* Second-order difference  is  denoted   $(1- B)^{2}$.

* \textit{Second-order  difference} is not  the  same  as  a \textit{second  difference},  which would  be  denoted $1- B^{2}$;

* In general,  a  $d$th-order difference can be written as
$$(1 - B)^{d} y_{t}.$$

* A seasonal difference followed by a first difference can be written as
$$ (1-B)(1-B^m)y_t\: .$$

## Backshift notation

The ``backshift'' notation is convenient because the terms can be multiplied
together to see the combined effect.
\begin{align*}
(1-B)(1-B^m)y_t &= (1 - B - B^m + B^{m+1})y_t \\
&= y_t-y_{t-1}-y_{t-m}+y_{t-m-1}.
\end{align*}\pause
For monthly data, $m=12$ and we obtain the same result as earlier.

# Autoregressive models

## Autoregressive models

\begin{block}{Autoregressive (AR) models:}
$$
  y_{t}  =  c  +  \phi_{1}y_{t - 1}  +  \phi_{2}y_{t - 2}  +  \cdots  +  \phi_{p}y_{t - p}  + \varepsilon_{t},
$$
where $\varepsilon_t$ is white noise.  This is a multiple regression with \textbf{lagged values} of $y_t$ as predictors.
\end{block}

```{r arp, echo=FALSE, fig.height=3}
set.seed(1)
p1 <- autoplot(10 + arima.sim(list(ar = -0.8), n = 100)) +
  ylab("") + ggtitle("AR(1)")
p2 <- autoplot(20 + arima.sim(list(ar = c(1.3, -0.7)), n = 100)) +
  ylab("") + ggtitle("AR(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```

## AR(1) model

\begin{block}{}
\centerline{$y_{t}    =   2 -0.8 y_{t - 1}  +  \varepsilon_{t}$}
\end{block}
\rightline{$\varepsilon_t\sim N(0,1)$,\quad $T=100$.}

```{r, echo=FALSE, out.width="50%", fig.height=2.2, fig.width=2.2}
p1
```

## AR(1) model

\begin{block}{}
\centerline{$y_{t}    =   c + \phi_1 y_{t - 1}  +  \varepsilon_{t}$}
\end{block}

* When $\phi_1=0$, $y_t$ is **equivalent to WN**
* When $\phi_1=1$ and $c=0$, $y_t$ is **equivalent to a RW**
* When $\phi_1=1$ and $c\ne0$, $y_t$ is **equivalent to a RW with drift**
* When $\phi_1<0$, $y_t$ tends to **oscillate between positive and negative values**.

## AR(2) model

\begin{block}{}
\centerline{$y_t = 8 + 1.3y_{t-1} - 0.7 y_{t-2} + \varepsilon_t$}
\end{block}
\rightline{$\varepsilon_t\sim N(0,1)$, \qquad $T=100$.}

```{r, fig.height=2.2, fig.width=2.2, out.width="50%", echo=FALSE}
p2
```

## Stationarity conditions

We normally restrict autoregressive models to stationary data, and then some constraints on the values of the parameters are required.

\begin{block}{General condition for stationarity}
Complex roots of $1-\phi_1 z - \phi_2 z^2 - \dots - \phi_pz^p$ lie outside the unit circle on the complex plane.
\end{block}\pause

* For $p=1$:  $-1<\phi_1<1$.
* For $p=2$:\newline $-1<\phi_2<1\qquad \phi_2+\phi_1 < 1 \qquad \phi_2 -\phi_1 < 1$.
* More complicated conditions hold for $p\ge3$.

# Moving Average models

## Moving Average (MA) models

\begin{block}{Moving Average (MA) models:}
$$
  y_{t}  =  c +  \varepsilon_t + \theta_{1}\varepsilon_{t - 1}  +  \theta_{2}\varepsilon_{t - 2}  +  \cdots  + \theta_{q}\varepsilon_{t - q},
$$
where $\varepsilon_t$ is white noise.
This is a multiple regression with  \textbf{past \emph{errors}}
as predictors. \emph{Don't confuse this with moving average smoothing!}
\end{block}

```{r maq, fig.height=2.5, echo=FALSE}
set.seed(2)
p1 <- autoplot(20 + arima.sim(list(ma = 0.8), n = 100)) +
  ylab("") + ggtitle("MA(1)")
p2 <- autoplot(arima.sim(list(ma = c(-1, +0.8)), n = 100)) +
  ylab("") + ggtitle("MA(2)")
gridExtra::grid.arrange(p1,p2,nrow=1)
```

## MA(1) model

\begin{block}{}
\centerline{$y_t = 20 + \varepsilon_t + 0.8 \varepsilon_{t-1}$}
\end{block}
\rightline{$\varepsilon_t\sim N(0,1)$,\quad $T=100$.}

```{r, fig.height=2.2, fig.width=2.2, out.width="50%", echo=FALSE}
p1
```

## MA(2) model

\begin{block}{}
\centerline{$y_t = \varepsilon_t -\varepsilon_{t-1} + 0.8 \varepsilon_{t-2}$}
\end{block}
\rightline{$\varepsilon_t\sim N(0,1)$,\quad $T=100$.}

```{r, fig.height=2.2, fig.width=2.2, out.width="50%", echo=FALSE}
p2
```

## Invertibility

* Invertible models have property that distant past has negligible effect on forecasts. Requires consraints on MA parameters.

\begin{block}{General condition for invertibility}
Complex roots of $1+\theta_1 z + \theta_2 z^2 + \dots + \theta_qz^q$ lie outside the unit circle on the complex plane.
\end{block}\pause\vspace*{-0.3cm}

* For $q=1$:  $-1<\theta_1<1$.
* For $q=2$:\newline $-1<\theta_2<1\qquad \theta_2+\theta_1 >-1 \qquad \theta_1 -\theta_2 < 1$.
* More complicated conditions hold for $q \ge 3$.

# Non-seasonal ARIMA models

## ARIMA models

\begin{block}{Autoregressive Moving Average models:}\vspace*{-0.2cm}
\begin{align*}
y_{t}  &=  c  +  \phi_{1}y_{t - 1}  +  \cdots  +  \phi_{p}y_{t - p} \\
& \hspace*{2.4cm}\text{} + \theta_{1}e_{t - 1} +  \cdots  + \theta_{q}e_{t - q} +  e_{t}.
\end{align*}
\end{block}\pause

* Predictors include both **lagged values of $y_t$ and lagged errors.**
* Conditions on coefficients ensure stationarity.
* Conditions on coefficients ensure invertibility.
\pause

### Autoregressive Integrated Moving Average models
* Combine ARMA model with **differencing**.
* $(1-B)^d y_t$ follows an ARMA  model.

## ARIMA models

\alert{Autoregressive Integrated Moving Average models}
\begin{block}{ARIMA($p, d, q$) model}
\begin{tabular}{rl}
AR:& $p =$  order of the autoregressive part\\
I: & $d =$  degree of first differencing involved\\
MA:& $q =$  order of the moving average part.
\end{tabular}
\end{block}

* White noise model:  ARIMA(0,0,0)
* Random walk:  ARIMA(0,1,0) with no constant
* Random walk with drift:  ARIMA(0,1,0) with \rlap{const.}
* AR($p$): ARIMA($p$,0,0)
* MA($q$): ARIMA(0,0,$q$)

## Backshift notation for ARIMA

* ARMA model:\vspace*{-1cm}\newline
\parbox{12cm}{\small\begin{align*}
\hspace*{-1cm}
y_{t}  &=  c + \phi_{1}By_{t} + \cdots + \phi_pB^py_{t}
           +  \varepsilon_{t}  +  \theta_{1}B\varepsilon_{t} + \cdots + \theta_qB^q\varepsilon_{t} \\
\hspace*{-1cm}
\text{or}\quad & (1-\phi_1B - \cdots - \phi_p B^p) y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t
\end{align*}}

* ARIMA(1,1,1) model:

\[
\begin{array}{c c c c}
(1 - \phi_{1} B) & (1  -  B) y_{t} &= &c + (1  + \theta_{1} B) \varepsilon_{t}\\
{\uparrow}  & {\uparrow}    &   &{\uparrow}\\
{\text{AR(1)}} & {\text{First}}   &     &{\text{MA(1)}}\\
& {\hbox to 0cm{\hss\text{difference}\hss}}\\
\end{array}
\]\pause
Written out:
$$y_t =   c + y_{t-1} + \phi_1 y_{t-1}- \phi_1 y_{t-2} + \theta_1\varepsilon_{t-1} + \varepsilon_t $$

## R model

\fontsize{13}{16}\sf

\begin{block}{Intercept form}
\centerline{$(1-\phi_1B - \cdots - \phi_p B^p) y_t' = c + (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t$}
\end{block}

\begin{block}{Mean form}
\centerline{$(1-\phi_1B - \cdots - \phi_p B^p)(y_t' - \mu) = (1 + \theta_1 B + \cdots + \theta_q B^q)\varepsilon_t$}
\end{block}

 * $y_t' = (1-B)^d y_t$
 * $\mu$ is the mean of $y_t'$.
 * $c = \mu(1-\phi_1 - \cdots - \phi_p )$.
 * R uses mean form.

## US personal consumption
\fontsize{11}{11}\sf
```{r, fig.height=3.8}
autoplot(uschange[,"Consumption"]) +
  xlab("Year") + ylab("Quarterly percentage change") +
  ggtitle("US consumption")
```

## US personal consumption
\fontsize{11}{13}\sf

```{r, echo=TRUE}
(fit <- auto.arima(uschange[,"Consumption"]))
```

```{r usconsumptioncoefs, echo=FALSE}
coef <- coefficients(fit)
intercept <- coef['intercept'] * (1-coef['ar1'] - coef['ar2'])
```

```{r, include=FALSE}
# Following line assumes forecast v8.4+
if(!identical(arimaorder(fit),c(p=2L,d=0L,q=2L)))
  stop("Different model from expected")
```

\pause\vfill

### ARIMA(2,0,2) model:
\centerline{$
  y_t = c + `r format(coef['ar1'], nsmall=3, digits=3)`y_{t-1}
          `r format(coef['ar2'], nsmall=3, digits=3)` y_{t-2}
          `r format(coef['ma1'], nsmall=3, digits=3)` \varepsilon_{t-1}
          + `r format(coef['ma2'], nsmall=3, digits=3)` \varepsilon_{t-2}
          + \varepsilon_{t},
$}
where $c= `r format(coef['intercept'], nsmall=3, digits=3)` \times (1 - `r format(coef['ar1'], nsmall=3, digits=3)` + `r format(-coef['ar2'], nsmall=3, digits=3)`) = `r format(intercept, nsmall=3, digits=3)`$
and $\varepsilon_t \sim N(0,`r format(fit$sigma2, nsmall=3, digits=3)`)$.

## US personal consumption
\fontsize{13}{14}\sf

```{r, echo=TRUE, fig.height=4}
fit %>% forecast(h=10) %>% autoplot(include=80)
```

## Understanding ARIMA models
\fontsize{14}{16}\sf

\begin{alertblock}{Long-term forecasts}
\centering\begin{tabular}{lll}
zero & $c=0,d=0$\\
non-zero constant & $c=0,d=1$ & $c\ne0,d=0$  \\
linear & $c=0,d=2$ & $c\ne0,d=1$ \\
quadratic & $c=0,d=3$ & $c\ne0,d=2$ \\
\end{tabular}
\end{alertblock}

### Forecast variance and $d$
  * The higher the value of $d$, the more rapidly the prediction intervals increase in size.
  * For $d=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.

## Understanding ARIMA models

### Cyclic behaviour
  * For cyclic forecasts,  $p\ge2$ and some restrictions on coefficients are required.
  * If $p=2$, we need $\phi_1^2+4\phi_2<0$. Then average cycle of length
\[
  (2\pi)/\left[\text{arc cos}(-\phi_1(1-\phi_2)/(4\phi_2))\right].
\]

# Estimation and order selection

## Maximum likelihood estimation

Having identified the model order, we need to estimate the
parameters $c$, $\phi_1,\dots,\phi_p$,
$\theta_1,\dots,\theta_q$.\pause

* MLE is very similar to least squares estimation obtained by minimizing
$$\sum_{t-1}^T e_t^2.$$
* The `Arima()` command allows CLS or MLE estimation.
* Non-linear optimization must be used in either case.
* Different software will give different estimates.

## Information criteria

\begin{block}{Akaike's Information Criterion (AIC):}
\centerline{$\text{AIC} = -2 \log(L) + 2(p+q+k+1),$}
where $L$ is the likelihood of the data,\newline
$k=1$ if $c\ne0$ and $k=0$ if $c=0$.
\end{block}\pause
\begin{block}{Corrected AIC:}
\[
\text{AICc} = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.
\]
\end{block}\pause
\begin{block}{Bayesian Information Criterion:}
\centerline{$\text{BIC} = \text{AIC} + \log(T)(p+q+k-1).$}
\end{block}
\pause\vspace*{-0.2cm}
\begin{alertblock}{}Good models are obtained by minimizing either the AIC, \text{AICc}\ or BIC\@. My preference is to use the \text{AICc}.\end{alertblock}

# ARIMA modelling in R

## How does auto.arima() work?

\begin{block}{A non-seasonal ARIMA process}
\[
\phi(B)(1-B)^dy_{t} = c + \theta(B)\varepsilon_t
\]
Need to select appropriate orders: \alert{$p,q, d$}
\end{block}

\alert{Hyndman and Khandakar (JSS, 2008) algorithm:}

  * Select no.\ differences \alert{$d$} and \alert{$D$} via KPSS test and seasonal strength measure.
  * Select \alert{$p,q$} by minimising AICc.
  * Use stepwise search to traverse model space.

## How does auto.arima() work?
\fontsize{12.5}{14}\sf

Step 1:
:  Select values of $d$ and $D$.

Step 2:
:  Select current model (with smallest AICc) from:\newline
ARIMA$(2,d,2)$\newline
ARIMA$(0,d,0)$\newline
ARIMA$(1,d,0)$\newline
ARIMA$(0,d,1)$
\pause\vspace*{-0.1cm}

Step 3:
:  Consider variations of current model:\vspace*{-0.2cm}

    * vary one of $p,q,$ from current model by \rlap{$\pm1$;}
    * $p,q$ both vary from current model by $\pm1$;
    * Include/exclude $c$ from current model.

  Model with lowest AICc becomes current model.

\begin{block}{}Repeat Step 3 until no lower AICc can be found.\end{block}

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
autoplot(internet)
```

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
internet %>% diff() %>% autoplot()
```

## Choosing an ARIMA model
\fontsize{12}{13}\sf

```{r, echo=TRUE, fig.height=4}
(fit <- auto.arima(internet))
```

## Choosing an ARIMA model
\fontsize{12}{13}\sf

```{r, echo=TRUE, fig.height=4}
(fit <- auto.arima(internet, stepwise=FALSE,
  approximation=FALSE))
```

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit, plot=TRUE)
```

## Choosing an ARIMA model
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4}
checkresiduals(fit, plot=FALSE)
```

## Choosing an ARIMA model

```{r, echo=TRUE, fig.height=4}
fit %>% forecast() %>% autoplot()
```

## Modelling procedure with `auto.arima`

1. Plot the data. Identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.
3. Use `auto.arima` to select a model.
4. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
5. Once the residuals look like white noise, calculate forecasts.

## Modelling procedure

\centerline{\includegraphics[height=8.cm]{Figure-8-10}}

## Seasonally adjusted electrical equipment
\fontsize{13}{15}\sf

```{r ee1, fig.height=3.3, echo=TRUE}
eeadj <- seasadj(stl(elecequip, s.window="periodic"))
autoplot(eeadj) + xlab("Year") +
  ylab("Seasonally adjusted new orders index")
```

## Seasonally adjusted electrical equipment

1. Time plot shows sudden changes, particularly big drop in 2008/2009 due to global economic environment. Otherwise nothing unusual and no need for  data adjustments.
2. No evidence of changing variance, so no Box-Cox transformation.
3. Data are clearly non-stationary, so we take first differences.

## Seasonally adjusted electrical equipment

```{r ee2, echo=TRUE, fig.height=4}
eeadj %>% diff() %>% autoplot()
```

## Seasonally adjusted electrical equipment
\fontsize{10}{11}\sf

```{r, echo=TRUE}
fit <- auto.arima(eeadj, stepwise=FALSE, approximation=FALSE)
summary(fit)
```

## Seasonally adjusted electrical equipment

6. ACF plot of residuals from ARIMA(3,1,1) model look like white noise.

```{r, echo=TRUE, fig.height=2.5}
ggAcf(residuals(fit))
```

## Seasonally adjusted electrical equipment
\fontsize{12}{14}\sf

```{r, echo=TRUE}
checkresiduals(fit, plot=FALSE)
```

## Seasonally adjusted electrical equipment

```{r, echo=TRUE}
fit %>% forecast() %>% autoplot()
```

# Lab session 16
##
\fontsize{48}{60}\sf\centering
**Lab Session 16**

## Prediction intervals

* Prediction intervals **increase in size with forecast horizon**.
* Calculations assume residuals are **uncorrelated** and **normally distributed**.
* Prediction intervals tend to be too narrow.
    * the uncertainty in the parameter estimates has not been accounted for.
    * the ARIMA model assumes historical patterns will not change during the forecast period.
    * the ARIMA model assumes uncorrelated future errors

## Bootstrapped prediction intervals
\fontsize{12}{13}\sf

```{r, echo=TRUE, fig.height=3.7}
fit %>% forecast(bootstrap=TRUE) %>% autoplot()
```

 * No assumption of normally distributed residuals.
# Seasonal ARIMA models

## Seasonal ARIMA models

| ARIMA | $~\underbrace{(p, d, q)}$ | $\underbrace{(P, D, Q)_{m}}$ |
| ----: | :-----------------------: | :--------------------------: |
|       | ${\uparrow}$              | ${\uparrow}$                 |
|       | Non-seasonal part         | Seasonal part of             |
|       | of the model              | of the model                 |

where $m =$ number of observations per year.

## Seasonal ARIMA models

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$  model (without constant)\pause
$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.
$$\pause

\setlength{\unitlength}{1mm}
\begin{footnotesize}
\begin{picture}(100,25)(-5,0)
\thinlines
{\put(5,22){\vector(0,1){6}}}
{\put(22,10){\vector(0,1){18}}}
{\put(38,22){\vector(0,1){6}}}
{\put(52,10){\vector(0,1){18}}}
{\put(77,22){\vector(0,1){6}}}
{\put(95,10){\vector(0,1){18}}}
{\put(-10,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{AR(1)}
                    \end{array}\right)$}}
{\put(12,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{AR(1)}
                    \end{array}\right)$}}
{\put(25,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{difference}
                    \end{array}\right)$}}
{\put(40,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{difference}
                    \end{array}\right)$}}
{\put(65,17){$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{MA(1)}
                    \end{array}\right)$}}
{\put(85,5){$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{MA(1)}
                    \end{array}\right)$}}
\end{picture}
\end{footnotesize}

\vspace*{10cm}

## Seasonal ARIMA models

E.g., ARIMA$(1, 1, 1)(1, 1, 1)_{4}$  model (without constant)
$$(1 - \phi_{1}B)(1 - \Phi_{1}B^{4}) (1 - B) (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B) (1 + \Theta_{1}B^{4})\varepsilon_{t}.
$$

All the factors can be multiplied out and the general model
written as follows:
\begin{align*}
y_{t}  &= (1 + \phi_{1})y_{t - 1} - \phi_1y_{t-2} + (1 + \Phi_{1})y_{t - 4}\\
&\text{}
 -  (1  + \phi_{1}  +  \Phi_{1} + \phi_{1}\Phi_{1})y_{t - 5}
 +  (\phi_{1}  +  \phi_{1} \Phi_{1}) y_{t - 6} \\
& \text{}  - \Phi_{1} y_{t - 8} +  (\Phi_{1}  +  \phi_{1} \Phi_{1}) y_{t - 9}
  - \phi_{1} \Phi_{1} y_{t  -  10}\\
  &\text{}
+    \varepsilon_{t} + \theta_{1}\varepsilon_{t - 1} + \Theta_{1}\varepsilon_{t - 4}  + \theta_{1}\Theta_{1}\varepsilon_{t - 5}.
\end{align*}
\vspace*{10cm}

## Common ARIMA models

The US Census Bureau uses the following models most often:\vspace*{0.5cm}

\begin{tabular}{|ll|}
\hline
ARIMA(0,1,1)(0,1,1)$_m$& with log transformation\\
ARIMA(0,1,2)(0,1,1)$_m$& with log transformation\\
ARIMA(2,1,0)(0,1,1)$_m$& with log transformation\\
ARIMA(0,2,2)(0,1,1)$_m$& with log transformation\\
ARIMA(2,1,2)(0,1,1)$_m$& with no transformation\\
\hline
\end{tabular}

## Understanding ARIMA models
\fontsize{14}{16}\sf

\begin{alertblock}{Long-term forecasts}
\centering\begin{tabular}{lll}
zero & $c=0,d+D=0$\\
non-zero constant & $c=0,d+D=1$ & $c\ne0,d+D=0$  \\
linear & $c=0,d+D=2$ & $c\ne0,d+D=1$ \\
quadratic & $c=0,d+D=3$ & $c\ne0,d+D=2$ \\
\end{tabular}
\end{alertblock}

### Forecast variance and $d+D$
  * The higher the value of $d+D$, the more rapidly the prediction intervals increase in size.
  * For $d+D=0$, the long-term forecast standard deviation will go to the standard deviation of the historical data.

## European quarterly retail trade

```{r, echo=TRUE, fig.height=3.6}
autoplot(euretail) +
  xlab("Year") + ylab("Retail index")
```

## European quarterly retail trade

```{r, echo=TRUE, fig.height=4}
euretail %>% diff(lag=4) %>% autoplot()
```

## European quarterly retail trade

```{r, echo=TRUE, fig.height=3.8}
euretail %>% diff(lag=4) %>% diff() %>%
  autoplot()
```

## European quarterly retail trade
\fontsize{11}{12}\sf

```{r euretail, echo=TRUE}
(fit <- auto.arima(euretail))
```
## European quarterly retail trade
\fontsize{11}{12}\sf

```{r euretail2, echo=TRUE}
(fit <- auto.arima(euretail, stepwise=TRUE,
  approximation=FALSE))
```

## European quarterly retail trade
\fontsize{13}{13}\sf

```{r, dependson='euretail2'}
checkresiduals(fit, test=FALSE)
```

## European quarterly retail trade
\fontsize{13}{13}\sf

```{r, dependson='euretail2'}
checkresiduals(fit, plot=FALSE)
```

## European quarterly retail trade
\fontsize{13}{13}\sf

```{r, dependson='euretail2'}
forecast(fit, h=36) %>% autoplot()
```

## Cortecosteroid drug sales

```{r h02, echo=FALSE}
lh02 <- log(h02)
tmp <- cbind("H02 sales (million scripts)" = h02,
             "Log H02 sales"=lh02)
autoplot(tmp, facets=TRUE) + xlab("Year") + ylab("")
```

## Cortecosteroid drug sales
\fontsize{13}{14}\sf

```{r h02b}
autoplot(diff(log(h02),12), xlab="Year",
  main="Seasonally differenced H02 scripts")
```

## Cortecosteroid drug sales
\fontsize{10}{14}\sf

```{r h02tryharder, echo=TRUE, fig.height=3.6}
(fit <- auto.arima(h02, lambda=0, max.order=9,
  stepwise=FALSE, approximation=FALSE))
```

## Cortecosteroid drug sales
\fontsize{13}{15}\sf

```{r, echo=TRUE, fig.height=4, dependson='h02tryharder'}
checkresiduals(fit)
```

## Cortecosteroid drug sales
\fontsize{13}{15}\sf

```{r, echo=FALSE, dependson='h02tryharder'}
checkresiduals(fit, plot=FALSE)
```

## Cortecosteroid drug sales
\fontsize{10}{12}\sf

Training data: July 1991 to June 2006

Test data: July 2006--June 2008

```r
getrmse <- function(x,h,...)
{
  train.end <- time(x)[length(x)-h]
  test.start <- time(x)[length(x)-h+1]
  train <- window(x,end=train.end)
  test <- window(x,start=test.start)
  fit <- Arima(train,...)
  fc <- forecast(fit,h=h)
  return(accuracy(fc,test)[2,"RMSE"])
}
getrmse(h02,h=24,order=c(3,0,0),seasonal=c(2,1,0),lambda=0)
getrmse(h02,h=24,order=c(3,0,1),seasonal=c(2,1,0),lambda=0)
getrmse(h02,h=24,order=c(3,0,2),seasonal=c(2,1,0),lambda=0)
getrmse(h02,h=24,order=c(3,0,1),seasonal=c(1,1,0),lambda=0)
getrmse(h02,h=24,order=c(3,0,1),seasonal=c(0,1,1),lambda=0)
getrmse(h02,h=24,order=c(3,0,1),seasonal=c(0,1,2),lambda=0)
getrmse(h02,h=24,order=c(3,0,1),seasonal=c(1,1,1),lambda=0)
getrmse(h02,h=24,order=c(3,0,3),seasonal=c(0,1,1),lambda=0)
getrmse(h02,h=24,order=c(3,0,2),seasonal=c(0,1,1),lambda=0)
getrmse(h02,h=24,order=c(2,1,3),seasonal=c(0,1,1),lambda=0)
getrmse(h02,h=24,order=c(2,1,4),seasonal=c(0,1,1),lambda=0)
getrmse(h02,h=24,order=c(2,1,5),seasonal=c(0,1,1),lambda=0)
getrmse(h02,h=24,order=c(4,1,1),seasonal=c(2,1,2),lambda=0)
```

## Cortecosteroid drug sales
\fontsize{12}{14}\sf

```{r, cache=TRUE, echo=FALSE}
models <- rbind(
  c(3,0,0,2,1,0),
  c(3,0,1,2,1,0),
  c(3,0,2,2,1,0),
  c(3,0,1,1,1,0),
  c(3,0,1,0,1,1),
  c(3,0,1,0,1,2),
  c(3,0,1,1,1,1),
  c(4,0,3,0,1,1),
  c(3,0,3,0,1,1),
  c(4,0,2,0,1,1),
  c(3,0,2,0,1,1),
  c(2,1,3,0,1,1),
  c(4,1,1,2,1,2),
  c(4,1,2,2,1,2),
  c(3,1,2,2,1,2),
  c(4,1,2,1,1,2),
  c(4,1,2,2,1,1),
  c(2,1,4,0,1,1),
  c(2,1,5,0,1,1))
h <- 24
train.end <- time(h02)[length(h02)-h]
test.start <- time(h02)[length(h02)-h+1]
train <- window(h02,end=train.end)
test <- window(h02,start=test.start)

rmse <- numeric(NROW(models))
modelname <- character(NROW(models))
for(i in seq(length(rmse)))
{
  fit <- Arima(train, order=models[i,1:3],
          seasonal=models[i,4:6], lambda=0)
  fc <- forecast(fit,h=h)
  rmse[i] <- accuracy(fc, test)[2,"RMSE"]
  modelname[i] <- as.character(fit)
}
k <- order(rmse)
knitr::kable(data.frame(Model=modelname[k],RMSE=rmse[k]),
             digits=4)
```

## Cortecosteroid drug sales

  * Models with lowest AICc values tend to give slightly better results than the other models.
  * AICc comparisons must have the same orders of differencing. But RMSE test set comparisons can involve any models.
  * Use the best model available, even if it does not pass all tests.

## Cortecosteroid drug sales
\fontsize{11}{12}\sf

```{r h02fa, echo=TRUE, fig.height=3.4}
fit <- Arima(h02, order=c(4,1,1), seasonal=c(2,1,2),
  lambda=0)
autoplot(forecast(fit)) + xlab("Year") +
  ylab("H02 sales (million scripts)") + ylim(0.3,1.8)
```

## Cortecosteroid drug sales
\fontsize{11}{12}\sf

```{r h02fb, echo=TRUE, fig.height=3.4}
fit <- Arima(h02, order=c(4,1,2), seasonal=c(2,1,2),
  lambda=0)
autoplot(forecast(fit)) + xlab("Year") +
  ylab("H02 sales (million scripts)") + ylim(0.3,1.8)
```

## Cortecosteroid drug sales
\fontsize{11}{12}\sf

```{r h02fc, echo=TRUE, fig.height=3.4}
fit <- Arima(h02, order=c(3,0,1), seasonal=c(0,1,2),
  lambda=0)
autoplot(forecast(fit)) + xlab("Year") +
  ylab("H02 sales (million scripts)") + ylim(0.3,1.8)
```

# Lab session 17
##
\fontsize{48}{60}\sf\centering
**Lab Session 17**

